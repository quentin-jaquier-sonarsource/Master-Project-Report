\section{Adding a new language to SLang}
\label{sec:new_language}

In this part, we are going to discuss the challenge to add a new language to \slang{}, with a general process and a concrete example with \emph{Scala}.

\subsection{General Process}
\label{subsec:general_procedure}

The addition of a new language follows a general process that can be described in a high-level way.
The first step is to choose a front-end to perform the parsing of the language.

\subsubsection{Front-end}
\label{subsubsec:front_end}

To choose a front-end, we have to take into consideration multiple points:
\begin{enumerate}
	\item \textbf{\textit{License}} \newline The tool developed will be open source, we have to use an existing front end with a compatible license.
	\item \textbf{\textit{Features}} \newline Static analysis requires specific needs, that is not necessarily provided by any front-end.	
	For example, we need a precise location of tokens to be able to report the issues to the user as precisely as possible.
	Another example is the comments, required for some of the common rule \ref{table:common_rules}, but typically removed early in a compiler front-end.
	\item \textbf{\textit{Maintenance}} \newline 
	The last criteria are completeness and maintenance.
	We want a tool regularly maintained and supporting the potential new feature of the language. 
\end{enumerate}

\subsubsection{Incrementally add new mapping and enable checks}
\label{subsubsec:new_mapping_and_enables_rules}

With the front-end, we now have access to the intermediate representation of the original language. 
We can start to work on running more and more checks. 
The work starts by looking at which rule we want to implement. 
Depending on the rule, we have to add the translation of more nodes from the original language to \slang{}. 
If the prerequisites for the front-end are respected, the initial effort of adding the mapping for a new node is an easy task.
We have to identify which nodes in the original language correspond to the node in \slang{}, and understand its structure to adapt it. 
Once we have added the nodes needed by the rule, and enabled it, we can then look at the results. 
This is a critical step, we must be sure that the rule makes sense on this new language, and that the reported issues are relevant. 

In many cases, this is where unexpected problems arise, they are usually due to an unknown language feature, a wrong approximation, and so on...
Hopefully, there are multiple ways to improve the results. 
The first one is to set up a parameter for the rule.
It will adapt the behavior of the rule depending on the original language, and even depending on the user input.
For example, all the rules of naming conventions have to be set up with the convention of the language and can be changed by the user thanks to a custom setup. 
Sometimes, the rule simply does not apply for the language. 
For example, if a language does not have a \emph{switch} statement, all the rules related to \emph{switch} will obviously not apply to this language.
The most challenging situation arises when the problem is not clear, where the approximation of the translation leads to issues that the tool could report, but the language specific features change the behavior of the check. 
These situations must be addressed case by case.

The problem of reporting relevant issues without too much noise is common in static analysis, it is often referred to as the precision and recall trade-off.

\subsubsection{Precision and Recall trade-off}
\label{subsubsec:precision_recall}

In static analysis, a common challenge is to deal with the precision and recall trade-off. When reporting an issue, we can be in two situations:

\begin{enumerate}
	\item \textbf{\textit{False Positive}} \newline The tool is reporting a non-existing bug.
	\item \textbf{\textit{True Positive}} \newline The tool is reporting a real bug. 
\end{enumerate}
Similarly, we can have false and true negatives, for real issue not reported and non-existing bug not reported, respectively. 

\emph{Precision} is the number of true positives, over the total number of issues reported by the tool ($ \emph{true positives} + \emph{false positives} $). \emph{Recall} is the number of true positive over the number of issues present in the code. 
Finding a good balance is a challenging task, in the first case, the programmer does not want to be surrounded with issues considered as irrelevant, this would hide the real issues and discredit the tool.
In the other extreme, a checker never reporting any issue will never report false positive, and will not be useful, because containing a lot of false negatives. 
Since there is no concrete solution to this trade-off, we are going to target a rate of less than \textbf{5\%} of false positive for our work.
This is an arbitrary choice, other tools like FindBugs \cite{Hovemeyer:2004:FBE:1052883.1052895} initially targeted a rate of less than 50\% of false positives for example. 
It mainly depends on the context in which the tool is used, an analysis of the software of an aircraft might want to have a high recall while a user working on a small project would like a precise tool. 
Targeting as little false positives as possible, accepting therefore more false negatives, but still report real issues is an important choice since it will greatly influence our design and implementation choices.

An important note is that we are not in the context of proving the absence of bugs, to provide a sound checker, but we want to reduce the best way possible their occurrences by reporting real problems, to be as complete as possible.

\subsection{A concrete example: Scala}
\label{subsec:concrete_example}

Scala is particularly interesting as it is the first functional language that is going to be added to \slang{}, it is going to help to understand how it supports different paradigms.
The first step is to find a good front-end.
Scalameta \cite{Scalameta:2019:Online} provides all the features needed, is widely used by the community and is intended to be used by static analysis tools. 
It seems to suit perfectly to the requirements for a good front-end.

\subsubsection{Incrementally mapping Scala to SLang}
\label{subsubsec:scala_to_slang}
The front-end have been chosen, we can use it to obtain a Scala abstract syntax tree from a Scala file. 
At this point, we have enough information to activate a first rule: file should parse.
If Scalameta is not able to parse the file, we report an issue.
The first step from this tree is to extract comments, and translate the tokens from Scalameta to \slang{}.
With this simple step, we are already able to enable new checks related to comments; the tracking of comments with \emph{TODO} and reporting commented code for example.
The second step is to start the translation. 
As in any compiler phase performing translation, the skeleton of the code will be a pattern matching on the current node. 
We will traverse the tree using a top-down approach.
The initial step is to map all nodes to \textbf{native trees}, they represent nodes we do not know anything about.
We still have access to the token of the native nodes; we can therefore activate the copy paste detection and the different metrics. 
In addition, all the rules related to the structure of a file can be enabled: length of line, tabulations, length of file.
With only little effort, we manage to enable 8 checks and provide a copy/paste detection and metrics. 
We will continue the effort by adding more and more nodes translations and activating more and more rules.

Most of the nodes from \slang{} have a direct equivalent in the Scalameta tree.
The translation effort is to make sure that the meaning of a node in the original language is the one intended in \slang{}, adapt it if not, and that the metadata is correctly handled.
Package declaration, literal and block are examples of nodes having a direct equivalent in \slang{} but surprisingly, more complex nodes such as if tree, and pattern match also fall into this category. 
The Scalameta nodes without equivalent in \slang{} will be translated to native tree.

The overall mapping stays pretty simple, we sometimes have to regroup
multiple children of the original node into one single native node, but it does
not contains any complex trick.

\subsubsection{Reducing the false positives}
\label{subsubsec:reducing_false_positives}

\slang{} is driven by checks, when we add a new node and enable a new one, we have to make sure that everything makes sense. 
For \emph{Scala}, some feature of the language greatly reduce the quality of the checks.
One quick but naive solution when facing false positive is to map the problematic node to a native node, to remove the problematic case.

\lstinputlisting[label={lst:pattern-match-fp},
caption=Pattern matching which can cause false positives]{code/pattern-match-fp.scala}

For example, listing \ref{lst:pattern-match-fp} shows a correct pattern matching, but with the current mapping, we only add the pattern ``a'' to the condition of the \emph{match} case, and not the guard (\emph{if(variable)}). 
This will incorrectly trigger the rule reporting identical branch body in a conditional structure. 
If we map the match case to native, this solves the problem but introduces false negative for other rules related to match tree.

Identifying which node can lead to false positives can be done during the mapping, but it is sometimes hard to feel where the problems will arise. 
To identify the potentially problematic cases, we can store in all nodes, the original node type from which it was created. 
After the translation, we can compute a mapping, from every original node to the node(s) in \slang{}. 
This gives a considerable list with all nodes present in Scalameta, which is not yet useful to identify potential problems. 
The first observation is that the majority of the nodes are mapped completely to native nodes. 
This is not a problem, we know we do not need all the nodes from the original language to perform our checks.
The more interesting cases are the original nodes mapped to a \slang{} node and a native node. 
All the rules using the nodes are conditionally translated are subject to false negatives.

\begin{table}[h]
	\centering
	\caption{Mapping from a node in Scalameta to the translated node in \slang{}}
	\label{table:interesting_mapping}
	\begin{tabular}{|c|c|}
		\hline
		DefnDef (1) &  FunctionDeclaration(90\%); Native(10\%) \\ \hline
		TermMatch (2) &  Matchl(70\%); BlockTree (21\%); Native(9\%) \\ \hline
		TermParam (3 \& 4) & Native(61\%); Parameter(39\%); \\ \hline
	\end{tabular}
\end{table}

Table \ref{table:interesting_mapping} shows the resulting table for Scala if we filter further to only keep the nodes where more than 10\% is mapped conditionally. 
This information can lead our research and lead to identify 4 potentials problematic situations.

\begin{enumerate}
	\captionsetup[lstlisting]{format=listingEnum, labelfont=white, textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize} }
	\item \textbf{\textit{Function with many parameter's clauses}} \newline 
	\lstinputlisting[label={lst:many-parameters},
	caption=Scala function with many parameter's clauses]{code/many-parameters.scala}
	
	Listing \ref{lst:many-parameters} shows an example of a Scala function with multiple parameter's clauses, common in Scala, but not necessarily in other languages. 
	Mapping the whole function to a native node is a big loss, we are not going to be able to run all the checks we could run inside functions.
	Adding the support for multiple lists to \slang{} is the first solution, but we have to make sure  that the benefit for this addition is worth the added complexity.
	In our situation, multiple lists support do not add any value to the checks.
	Indeed, none of them would use this information, we should not add it to the language.
	A second solution is to merge all the parameters into one single list.
	This solution works fine, we recover the possibility to run all checks applying to functions.
	However, the check limiting the number of parameters raises some unexpected issues.
	If we limit the number to one single argument, the code from listing \ref{lst:many-parameters} will raise an issue.
	The problem is the implicit keyword, it is here to avoid giving this argument when calling this function, one can argue that implicit parameters should not be accounted for.
	Despite this concern, we chose this solution, a user can always configure the limit if he thinks that the checks raise unexpected issues.
	
	This case describes the challenge when implementing static analyzer.
	There are often multiple solutions, not really complex in themselves, but it requires a good understanding of the whole ecosystem, from the original language keyword \emph{implicit} to the final implementation on \slang{}, to be able to produce results of good quality.
	
	\item \textbf{\textit{Match statement with at least one conditional case}} \newline 
	The case of listing \ref{lst:pattern-match-fp} seen previously, also appear in the list. 
	In the current situation, the whole match statement is converted to a native tree if at least one conditional case is present. 
	The granularity of this solution is not fine enough.
	Indeed, we still want to be able to run the different checks related to match tree, even if one branch has an unknown structure.
	The solution chosen is to wrap the case tree inside a native node in the case where a guard is present.
	This fine granularity is far better.
	We are now able to compare the body of the different cases, to report duplicate ones, and to compare the pattern.
	
	\item \textbf{\textit{Function parameters with default value}} 
	\lstinputlisting[label={lst:scala-default-modifier},
	caption=Scala function with default value]{code/scala-default-modifier.scala}
	
	In listing \ref{lst:scala-default-modifier}, we can see function parameters with default value. Once again, the first question is to know if adding support for such construction is worth it. 
	As \emph{Ruby}, \emph{Scala} and \emph{Kotlin} have default value, it makes sense to add the support to \slang{}. 
	This new structure adds new issues on bad naming convention, hard-coded IP addresses inside default values, unused parameters, and few others, comforting our choice that it was worth to support it.

	\item \textbf{\textit{Function parameters with modifier}} 
	\lstinputlisting[label={lst:scala-modifiers},
	caption=Scala function with implicit modifier]{code/scala-modifiers.scala} 
	
	Listing \ref{lst:scala-modifiers} shows a problematic situation with the rule checking for unused parameters in Scala. In this case, the parameter \emph{param} seems to be unused in \emph{f} while it is implicitly passed to \emph{g}.
	This is the reason why we mapped parameter to native in the first place.
	At first glance, we might think that \emph{Scala} is the only language having an \emph{implicit} modifier, but if we generalize the idea, we can expect other modifier and even annotation at this place. 
	Annotation is more popular and could be useful in the future, as we will discuss in subsection \ref{subsubsec:guided_by_annotation}.
	The solution we chose is to add the support for modifiers in \slang{}, to map both modifiers and annotation to this node.
	We are currently supporting only a fraction of the modifier possible, the majority of them, including \emph{implicit}, are going to be native nodes.
	We are now able to adapt the check for unused variable, but not reporting issues on variable with a modifier. 
	We might miss some real issues since the modifier is a native node unrelated to the problem.
	However, the approximation performed already provides good results, avoiding obvious false positives.
\end{enumerate}

